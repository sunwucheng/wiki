<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>ML.DL | MYTH&#39;s Wiki</title>
    
    
        <meta name="keywords" content="ML.DL" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="来源：动手深度学习PyTorch版-李沐 1 课程安排 深度学习基础：线性神经网络, 多层感知机 卷积神经网络：LeNet, AlexNet, VGG, Inception, ResNet 循环神经网络：RNN, GRU, LSTM, seq2seq 注意力机制：Attention, Transformer 优化算法：SGD, Momentum, Adam 高性能计算：并行，多GPU，分布式 计算">
<meta property="og:type" content="article">
<meta property="og:title" content="ML.DL">
<meta property="og:url" content="https://sunwucheng.com/wiki/CS&EE/ML.DL/index.html">
<meta property="og:site_name" content="MYTH&#39;s Wiki">
<meta property="og:description" content="来源：动手深度学习PyTorch版-李沐 1 课程安排 深度学习基础：线性神经网络, 多层感知机 卷积神经网络：LeNet, AlexNet, VGG, Inception, ResNet 循环神经网络：RNN, GRU, LSTM, seq2seq 注意力机制：Attention, Transformer 优化算法：SGD, Momentum, Adam 高性能计算：并行，多GPU，分布式 计算">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-04-30T16:00:00.000Z">
<meta property="article:modified_time" content="2023-11-19T02:37:45.000Z">
<meta property="article:author" content="sunwucheng">
<meta name="twitter:card" content="summary">
    

    

    
        <link rel="icon" href="/wiki/favicon.ico" />
    

    
<link rel="stylesheet" href="/wiki/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/wiki/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/wiki/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/wiki/css/style.css">

    
<script src="/wiki/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/wiki/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/wiki/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/wiki/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/wiki/atom.xml" title="MYTH's Wiki" type="application/atom+xml">
</head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/wiki/" id="logo">
                <i class="logo"></i>
                <span class="site-title">MYTH&#39;s Wiki</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="https://sunwucheng.com">MYTH</a>
                
                    <a class="main-nav-link" href="https://sunwucheng.com/wiki">WIKI</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/wiki/',
        CONTENT_URL: '/wiki/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/wiki/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="https://sunwucheng.com">MYTH</a></td>
                
                    <td><a class="main-nav-link" href="https://sunwucheng.com/wiki">WIKI</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-up fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" style="display: block;"> 
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            CS&EE
                        </a>
                         <ul class="unstyled" id="tree" style="display: block;">  <li class="file"><a href="/wiki/CS&EE/C/">C</a></li>  <li class="file"><a href="/wiki/CS&EE/Git/">Git</a></li>  <li class="file"><a href="/wiki/CS&EE/Linux/">Linux</a></li>  <li class="file"><a href="/wiki/CS&EE/Script/">Script</a></li>  <li class="file active"><a href="/wiki/CS&EE/ML.DL/">ML.DL</a></li>  <li class="file"><a href="/wiki/CS&EE/Arduino/">Arduino</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            English
                        </a>
                         <ul class="unstyled" id="tree" style="display: block;">  <li class="file"><a href="/wiki/English/CATTI/">CATTI</a></li>  <li class="file"><a href="/wiki/English/IELTS/">IELTS</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Interest
                        </a>
                         <ul class="unstyled" id="tree" style="display: block;">  <li class="file"><a href="/wiki/Interest/History/">History</a></li>  <li class="file"><a href="/wiki/Interest/Geography/">Geography</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Materials
                        </a>
                         <ul class="unstyled" id="tree" style="display: block;">  <li class="file"><a href="/wiki/Materials/Electrons/">Electrons</a></li>  <li class="file"><a href="/wiki/Materials/Materials/">Materials</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Research
                        </a>
                         <ul class="unstyled" id="tree" style="display: block;">  <li class="file"><a href="/wiki/Research/IDB_matrix_wear/">IDB_matrix_wear</a></li>  <li class="file"><a href="/wiki/Research/IDB_diamond_damage/">IDB_diamond_damage</a></li>  <li class="file"><a href="/wiki/Research/IDB_drilling_signal/">IDB_drilling_signal</a></li>  <li class="file"><a href="/wiki/Research/IDB_formula_design/">IDB_formula_design</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/wiki/index/">Introduction</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-CS&amp;EE/ML.DL" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/wiki/categories/CS-EE/">CS&EE</a>
    </div>

                        
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/CS&EE/ML.DL/">
            <time datetime="2020-04-30T16:00:00.000Z" itemprop="datePublished">2020-05-01</time>
        </a>
    </div>


                        
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            ML.DL
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <p><strong>来源：<a target="_blank" rel="noopener" href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497">动手深度学习PyTorch版-李沐</a></strong></p>
<h1 id="1-课程安排"><a href="#1-课程安排" class="headerlink" title="1 课程安排"></a>1 课程安排</h1><ul>
<li>深度学习基础：线性神经网络, 多层感知机</li>
<li>卷积神经网络：LeNet, AlexNet, VGG, Inception, ResNet</li>
<li>循环神经网络：RNN, GRU, LSTM, seq2seq</li>
<li>注意力机制：Attention, Transformer</li>
<li>优化算法：SGD, Momentum, Adam</li>
<li>高性能计算：并行，多GPU，分布式</li>
<li>计算机视觉：目标检测，语义分割</li>
<li>自然语言处理：词嵌入，BERT</li>
</ul>
<h1 id="2-深度学习介绍"><a href="#2-深度学习介绍" class="headerlink" title="2 深度学习介绍"></a>2 深度学习介绍</h1><p>AI地图、图片分类、物体检测和分割、样式迁移、人脸合成、文字生成图片、文字生成、无人驾驶<br>案例研究——广告点击：预测与训练，完整的故事</p>
<h1 id="3-本地安装（略）"><a href="#3-本地安装（略）" class="headerlink" title="3 本地安装（略）"></a>3 本地安装（略）</h1><h1 id="4-数据操作"><a href="#4-数据操作" class="headerlink" title="4 数据操作"></a>4 数据操作</h1><h2 id="4-1-N维数组"><a href="#4-1-N维数组" class="headerlink" title="4.1 N维数组"></a>4.1 N维数组</h2><p>N维数组是机器学习和神经网络的主要数据结构，样例：</p>
<ul>
<li>0-d（标量）：一个类别</li>
<li>1-d（向量）：一个特征向量</li>
<li>2-d（矩阵）：一个样本——特征矩阵</li>
<li>3-d：RGB图片（宽×高×通道）</li>
<li>4-d：一个RGB图片批量batch（批量大小×宽×高×通道）</li>
<li>5-d：一个视频批量（批量大小×时间×宽×高×通道）</li>
</ul>
<h2 id="4-2-创建数组"><a href="#4-2-创建数组" class="headerlink" title="4.2  创建数组"></a>4.2  创建数组</h2><p>创建数组需要</p>
<ul>
<li>形状：例如3×4矩阵</li>
<li>类型：每个元素的数据类型，如32位浮点数</li>
<li>数值：每个元素的值，如全是0或随机数</li>
</ul>
<h2 id="4-3-访问元素"><a href="#4-3-访问元素" class="headerlink" title="4.3 访问元素"></a>4.3 访问元素</h2><p>对m×n矩阵，有第0,1,2…i,…(m-1)行、第0,1,2…j,…(n-1)列</p>
<ul>
<li>一个元素：[i,j]</li>
<li>一行元素：[i,:]</li>
<li>一列元素：[:,j]</li>
<li>一子区域：[$i_1$:$i_2$,j:]</li>
<li>一跳行子区域：[::i,::j]（从第0行0列元素开始，每i个数据取行、每j个数据取列访问元素）</li>
</ul>
<h2 id="4-4-数据操作实现"><a href="#4-4-数据操作实现" class="headerlink" title="4.4 数据操作实现"></a>4.4 数据操作实现</h2><p>张量表示一个数值组成的数组，这个数组可能有多个维度<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; torch.arrange(12)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></p>
<p>可以通过张量的<code>shape</code>属性来访问张量的形状和张量中元素的总数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br><span class="line">x.numel()</span><br></pre></td></tr></table></figure></p>
<p>可以调用<code>reshape</code>函数来改变一个张量的形状而不改变元素数量和元素值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; x.reshape(3,4)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></p>
<p>使用全0、全1、其他常量或者从特定分布中随机采样的数字<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((2,3,4))</span><br><span class="line">torch.ones((2,3,4))</span><br></pre></td></tr></table></figure></p>
<p>通过提供包含数值的Python列表（或嵌套列表）来为所需张量中的每个元素赋予确定值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])</span><br></pre></td></tr></table></figure></p>
<p>常见的标准算术运算符都可以升级为按元素运算<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; torch.tensor([1.0,2,4,8])</span><br><span class="line">y &#x3D; torch.tensor([2,2,2,2])</span><br><span class="line">x+y, x-y, x*y, x&#x2F;y, x**y</span><br></pre></td></tr></table></figure></p>
<p>按元素方式应用更多的计算（指数运算）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure></p>
<p>把多个张量连结在一起（在第0维、第1维）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; torch.arange(12, dtype&#x3D;torch.float32).reshape((3,4))</span><br><span class="line">Y &#x3D; torch.tensor([[2.0,1,4,3],[1,2,3,4),[4,3,2,1]])</span><br><span class="line">tensor.cat((X,Y),dim&#x3D;0), torch.cat((X,Y),dim&#x3D;1)</span><br></pre></td></tr></table></figure></p>
<p>通过逻辑运算符构建二元张量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D;&#x3D; Y</span><br></pre></td></tr></table></figure></p>
<p>对张量中的所有元素进行求和会产生一个只有一个元素的张量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.sum()</span><br></pre></td></tr></table></figure></p>
<p>即使形状不同，仍然可以通过调用广播机制(broadcasting mechanism)来执行按元素操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; torch.arange(3).reshape((3,1))</span><br><span class="line">b &#x3D; torch.arange(2).reshape((1,2))</span><br><span class="line">a, b, a+b</span><br></pre></td></tr></table></figure></p>
<p>可以用<code>[-1]</code>选择最后一个元素，用<code>[1:3]</code>选择第二个和第三个元素（第1个、第2个元素）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[-1], Y[1:3]</span><br></pre></td></tr></table></figure></p>
<p>除读取外，还可以通过指定索引来将元素写入矩阵<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[1,2] &#x3D; 9</span><br></pre></td></tr></table></figure></p>
<p>为多个元素赋相同的值，只需索引所有元素然后为其赋值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[0:2,:] &#x3D; 12</span><br><span class="line">X</span><br></pre></td></tr></table></figure></p>
<p>运行一些操作可能导致新结果分配内存<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before &#x3D; id(Y)</span><br><span class="line">Y &#x3D; Y+X</span><br><span class="line">id(Y) &#x3D;&#x3D; before</span><br></pre></td></tr></table></figure></p>
<p>执行原地操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z &#x3D; torch.zeros_like(Y)</span><br><span class="line">print(&#39;id(Z):&#39;, id(Z))</span><br><span class="line">Z[:] &#x3D; X+Y</span><br><span class="line">print(&#39;id(Z):&#39;, id(Z))</span><br></pre></td></tr></table></figure></p>
<p>若后续计算中美哟重复使用X，可以使用<code>X[:] = X+Y</code>或<code>X += Y</code>来减少操作的内存开销<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before &#x3D; id(X)</span><br><span class="line">X +&#x3D; Y</span><br><span class="line">id(X) &#x3D;&#x3D; before</span><br></pre></td></tr></table></figure></p>
<p>转换为NumPy张量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A &#x3D; X.numpy()</span><br><span class="line">B &#x3D; torch.tensor(A)</span><br><span class="line">type(A), type(B)</span><br></pre></td></tr></table></figure></p>
<p>将大小为1的张量转换成Python标量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; torch.tensor([3.5])</span><br><span class="line">a, a.item(), float(a), int(a)</span><br></pre></td></tr></table></figure></p>
<h1 id="5-数据预处理"><a href="#5-数据预处理" class="headerlink" title="5 数据预处理"></a>5 数据预处理</h1><p>创建一个人工数据集，并存储在csv文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(&#39;..&#39;, &#39;data&#39;), exist_ok&#x3D;True)</span><br><span class="line">data_file &#x3D; os.path.join(&#39;..&#39;, &#39;data&#39;, &#39;house_tiny.csv&#39;)</span><br><span class="line">with open(data_file, &#39;w&#39;) as f:</span><br><span class="line">    f.write(&#39;NumRooms,Alley,Price\n&#39;)</span><br><span class="line">    f.write(&#39;NA,Pave,127500\n&#39;)</span><br><span class="line">    f.write(&#39;2,NA,106000\n&#39;)</span><br><span class="line">    f.write(&#39;4,NA,178100\n&#39;)</span><br><span class="line">    f.write(&#39;NA,NA,140000\n&#39;)</span><br></pre></td></tr></table></figure></p>
<p>从创建的csv文件中加载原始数据集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 如果没有安装pandas，只需取消对以下行的注释：</span><br><span class="line"># !pip install pandas</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">data &#x3D; pd.read_csv(data_file)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure></p>
<p>为处理缺失的数据，典型方法包括插值和删除，这里考虑差值方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs &#x3D; data.iloc[:, 0:2], data.iloc[:, 2]</span><br><span class="line">inputs &#x3D; inputs.fillna(inputs.mean())</span><br><span class="line">print(inputs)</span><br></pre></td></tr></table></figure></p>
<p>对于 inputs 中的类别值或离散值，将 “NaN” 视为一个类别<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs &#x3D; pd.get_dummies(inputs, dummy_na&#x3D;True)</span><br><span class="line">print(inputs)</span><br></pre></td></tr></table></figure></p>
<p>现在 inputs 和 outputs 中的所有条目都是数值类型，它们可以转换为张量格式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">X, y &#x3D; np.array(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure></p>
<p>其他：<code>reshape</code>？？？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; torch.arange(12)</span><br><span class="line">b &#x3D; a.reshape((3,4))</span><br><span class="line">b[:] &#x3D; 2</span><br><span class="line">a</span><br></pre></td></tr></table></figure></p>
<p>其他：<code>tensor</code>（数学概念，张量，深度学习框架作者数学不太行混用的）和<code>array</code>（计算机概念，数组，ndarray是多维数组）</p>
<h1 id="5-线性代数"><a href="#5-线性代数" class="headerlink" title="5 线性代数"></a>5 线性代数</h1><h2 id="5-1-向量"><a href="#5-1-向量" class="headerlink" title="5.1 向量"></a>5.1 向量</h2><ul>
<li>点乘：$a^{\top} b=\sum_{i} a_{i} b_{i}$</li>
<li>正交：$a^{\top} b=\sum_{i} a_{i} b_{i}=0$</li>
</ul>
<h2 id="5-2-矩阵"><a href="#5-2-矩阵" class="headerlink" title="5.2 矩阵"></a>5.2 矩阵</h2><ul>
<li>乘法（矩阵乘以向量）：$c=A b$ where $c_{i}=\sum_{j} A_{i j} b_{j}$</li>
<li>乘法（矩阵乘以矩阵）：$C=A B$ where $C_{i k}=\sum_{j} A_{i j} B_{j k}$</li>
<li>范数：$c=A \cdot b$ hence $|c| \leq|A| \cdot|b|$<ul>
<li>矩阵范数：最小的满足上面公式的值</li>
<li>Frobenius范数：$|A|_{\text {Frob }}=\left[\sum_{i j} A_{i j}^{2}\right]^{\frac{1}{2}}$</li>
</ul>
</li>
<li>特殊矩阵<ul>
<li>对称和反对称：$A_{i j}=A_{j i}$ and $A_{i j}=-A_{j i}$</li>
<li>正定矩阵：$|x|^{2}=x^{\top} x \geq 0$ generalizes to $x^{\top} A x \geq 0$</li>
<li>正交矩阵：$U U^{\top}=\mathbf{1}$</li>
<li>置换矩阵：$P$ where $P_{i j}=1$ if and only if $j=\pi(i)$</li>
</ul>
</li>
<li>特征向量和特征值：特征向量是不被矩阵改变方向的向量，对称矩阵总能找到特征向量</li>
</ul>
<h2 id="5-3-线性代数实现"><a href="#5-3-线性代数实现" class="headerlink" title="5.3 线性代数实现"></a>5.3 线性代数实现</h2><p>标量由只有一个元素的张量表示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x &#x3D; torch.tensor([3.0])</span><br><span class="line">y &#x3D; torch.tensor([2.0])</span><br><span class="line">x+y, x*y, x&#x2F;y, x**y</span><br></pre></td></tr></table></figure></p>
<p>可将向量视为标量值组成的列表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; torch.arange(4)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></p>
<p>通过张量的索引来访问任一元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[3]</span><br></pre></td></tr></table></figure></p>
<p>访问张量的长度<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(x)</span><br></pre></td></tr></table></figure></p>
<p>只有一个轴的张量，形状只有一个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure></p>
<p>通过指定两个分量m和n来创建一个形状为m×n的矩阵<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A &#x3D; torch.arange(20).reshape(5,4)</span><br><span class="line">A</span><br></pre></td></tr></table></figure></p>
<p>矩阵的转置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure></p>
<p>对称矩阵(symmetric matrix)A等于其转置：$\mathbf{A}=\mathbf{A}^{\top}$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">B &#x3D; torch.tensor([[1,2,3],[2,0,4],[3,4,5]])</span><br><span class="line">B</span><br><span class="line">B &#x3D;&#x3D; B.T</span><br></pre></td></tr></table></figure></p>
<p>构建具有更多轴的数据结构<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; torch.arange(24).reshape(2,3,4)</span><br><span class="line">X</span><br></pre></td></tr></table></figure></p>
<p>给定具有相同形状的任何两个张量，任何按元素二元计算的结果都将是相同形状的张量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A &#x3D; torch.arange(20, dtype.float32).reshape(5,4)</span><br><span class="line">B &#x3D; A.clone()</span><br><span class="line">A, A+B</span><br></pre></td></tr></table></figure></p>
<p>两个矩阵的按元素乘法称为哈达玛积(Hadamard product)（数学符号<code>⊙</code>）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A * B</span><br><span class="line">a &#x3D; 2</span><br><span class="line">X &#x3D; torch.arange(24).reshape(2,3,4)</span><br><span class="line">a+X, (a*X).shape</span><br></pre></td></tr></table></figure></p>
<p>计算其元素的和<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; torch.arange(4, dtype&#x3D;torch.float32)</span><br><span class="line">x, x.sum()</span><br></pre></td></tr></table></figure></p>
<p>表示任意形状张量的元素和<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A &#x3D; torch.arange(20*2).reshape(2,5,4)</span><br><span class="line">A.shape, A.sum()</span><br></pre></td></tr></table></figure></p>
<p>指定求和汇总张量的轴<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 &#x3D; A.sum(axis&#x3D;0)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">A_sum_axis1 &#x3D; A.sum(axis&#x3D;1)</span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br><span class="line"></span><br><span class="line">A.sum(axis&#x3D;[0,1])</span><br><span class="line">A.sum(axis&#x3D;[0,1]).shape</span><br></pre></td></tr></table></figure></p>
<p>一个与求和相关的量是平均值(mean/average)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A.mean(), A.sum(), A.numel()</span><br><span class="line">A.mean(axis&#x3D;0), A.sum(axis&#x3D;0)&#x2F;A.shape[0]</span><br></pre></td></tr></table></figure></p>
<p>计算总和或均值保持轴数不变<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum_A &#x3D; A.sum(axis&#x3D;1, keepdims&#x3D;True)</span><br><span class="line">sum_A</span><br></pre></td></tr></table></figure></p>
<p>通过广播将A除以sum_A<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A&#x2F;sum_A</span><br></pre></td></tr></table></figure></p>
<p>点积是相同位置的按元素乘积的和<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; torch.ones(4, dtype&#x3D;torch.float32)</span><br><span class="line">x, y, torch.dot(x,y)</span><br></pre></td></tr></table></figure></p>
<p>也可以通过执行按元素乘法，然后进行求和来表示两个向量的点积<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sum(x*y)</span><br></pre></td></tr></table></figure></p>
<p>矩阵向量积Ax是一个长度为m的列向量，其$i^{\text {th }}$元素是点积$\mathbf{a}_{i}^{\top} \mathbf{x}$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A,x)</span><br></pre></td></tr></table></figure></p>
<p>可以将矩阵乘法AB看作是简单执行m次矩阵-向量积，然后将结果拼接在一起，形成一个n×m矩阵<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B &#x3D; torch.ones(4,3)</span><br><span class="line">torch.mm(A,B)</span><br></pre></td></tr></table></figure></p>
<p>$L_{2}$范数是向量元素平方和的平方根：$|\mathbf{x}|_{2}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u &#x3D; torch.tensor([3.0,-4.0])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure></p>
<p>矩阵的弗罗贝尼乌斯范数(Frobenius norm)是矩阵元素的平方和的平方根：$|\mathbf{X}|_{F}=\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} x_{i j}^{2}}$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((4,9)))</span><br></pre></td></tr></table></figure></p>
<p>[补充]按特定轴求和</p>
<ul>
<li>shape: [5,4] i.e., axis: 0,1<ul>
<li>axis=0, sum: [4]</li>
<li>axis=1, sum: [5]</li>
</ul>
</li>
<li>shape: [2,5,4] i.e., axis: 0,1,2<ul>
<li>axis=1, sum: [2,4]</li>
<li>axis=2, sum: [2,5]</li>
<li>axis=[1,2], sum: [2]</li>
</ul>
</li>
</ul>
<p>keepdims=True</p>
<ul>
<li>shape: [2,5,4] i.e., axis: 0,1,2<ul>
<li>axis=1, sum: [2,1,4]</li>
<li>axis=[1,2], sum: [2,1,1]</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a &#x3D; torch.ones((2,5,4))</span><br><span class="line">a.shape</span><br><span class="line"></span><br><span class="line">a.sum(axis&#x3D;0).shape</span><br><span class="line">a.sum(axis&#x3D;1).shape</span><br><span class="line">a.sum(axis&#x3D;[0,2]).shape</span><br><span class="line"></span><br><span class="line">a.sum(axis&#x3D;1,keepdims&#x3D;True).shape</span><br><span class="line">a.sum(axis&#x3D;[0,2],keepdims&#x3D;True).shape</span><br></pre></td></tr></table></figure>
<p>问题：torch不区分行向量和列向量吗？<br>一个数学概念的向量对计算机来讲就是一个一维数组，可以用一个二维矩阵来区分行向量和列向量</p>
<ul>
<li>行向量：矩阵的行数为1，列数为n</li>
<li>列向量：矩阵的行数为n，列数为1</li>
</ul>
<h1 id="6-矩阵计算"><a href="#6-矩阵计算" class="headerlink" title="6 矩阵计算"></a>6 矩阵计算</h1><h2 id="6-1-标量导数——导数是切线的斜率"><a href="#6-1-标量导数——导数是切线的斜率" class="headerlink" title="6.1 标量导数——导数是切线的斜率"></a>6.1 标量导数——导数是切线的斜率</h2><h2 id="6-2-亚导数——将导数拓展到不可微的函数"><a href="#6-2-亚导数——将导数拓展到不可微的函数" class="headerlink" title="6.2 亚导数——将导数拓展到不可微的函数"></a>6.2 亚导数——将导数拓展到不可微的函数</h2><h2 id="6-3-梯度——将导数拓展到向量"><a href="#6-3-梯度——将导数拓展到向量" class="headerlink" title="6.3 梯度——将导数拓展到向量"></a>6.3 梯度——将导数拓展到向量</h2><h2 id="6-4-拓展到矩阵"><a href="#6-4-拓展到矩阵" class="headerlink" title="6.4 拓展到矩阵"></a>6.4 拓展到矩阵</h2>
            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/CS&EE/Arduino/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    Arduino
                
            </div>
        </a>
    
    
        <a href="/wiki/CS&EE/Script/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Script</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            sunwucheng &copy; 2025 
             | Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>-<a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/wiki/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/wiki/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/wiki/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/wiki/js/main.js"></script>


    </div>
</body>
</html>